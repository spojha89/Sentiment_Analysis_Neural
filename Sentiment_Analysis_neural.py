# -*- coding: utf-8 -*-
"""Subhra_Ojha_Module13.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xzD0HfGPyOLjs38esFXoIWVU97ak1j54
"""

#Import modules

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,Embedding,SpatialDropout1D,Flatten,LSTM,Bidirectional,GlobalAveragePooling1D
from tensorflow.keras.layers import Dropout

from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from sklearn.metrics import classification_report

import warnings
from sklearn.preprocessing import LabelEncoder

# Pandas column width
pd.set_option('max_colwidth', None)

# Seaborn palette
sns.set_style('whitegrid')
sns.set_palette('viridis')

# Random number seed
seed = 343

# Warnings
warnings.filterwarnings('ignore')

#Read files from google drive

from google.colab import drive
drive.mount('/content/drive')

"""Question-

This criterion is linked to a Learning OutcomeRead the data from 'data.csv and 'data_test.csv'
"""

data=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data-4.csv')
data_test=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data_test.csv')

print(data.head(10))

print(data_test.head(10))

#Add headers

data.columns=['text','label']
data_test.columns=['text','label']

print('Shape of data:',data.shape)
print('Shape of test data:',data_test.shape)

data.describe()

data.isnull().sum()

#Check duplicates

if data.duplicated().sum()>0:
  print('There are duplicates')
else:
  print('There are no duplicates')

#Find the duplicates

data[data.duplicated()]

#Remove duplicates

data.drop_duplicates(inplace=True)

data.duplicated().sum()

"""The dataset has 2 columns i.e. text and then level as sentiments like positive and negative.

There are 96 duplicates which are now removed.
"""

data['label'].value_counts()

data = data.reset_index(drop = True)

"""The positive and negative counts are almost equally distributed. So there is not much skewness in the dataset.

Question-

This criterion is linked to a Learning Outcome.

Vectorization

Convert the text into padded sequences and also convert labels into numbers
"""

#Find length of each text column

def sent_len(dataframe, col_to_assess):

    '''
    Inputs:
    dataframe --> Array
    col_to_assess: str, name of column with text whose length is to be determined

    Output:
    list: containing the total number of words in the specified column for each row of the specified dataframe
    '''

    sent_len_list = []

    for i in range(dataframe.shape[0]):

        text = dataframe[col_to_assess][i].strip()  # Remove leading and trailing whitespaces
        split_sent = text.split(' ')  # Use space as the delimiter
        split_sent_len = len(split_sent)
        sent_len_list.append(split_sent_len)

        # split_sent = dataframe[col_to_assess][i].split('')
        # split_sent_len = len(split_sent)
        # sent_len_list.append(split_sent_len)

    return sent_len_list

# Using the 'sent_len' function created earlier to calculate the length of the text in each row

og_sent_len = sent_len(data, 'text')

# Appending the list above to the 'data' dataframe
data['Original Sentence Length'] = og_sent_len

# Inspecting the dataframe
data.head()

#Check max, min, mean length of original sentence

print('Max length of original sentence:',data['Original Sentence Length'].max())
print('Min length of original sentence:',data['Original Sentence Length'].min())
print('Mean length of original sentence:',round(data['Original Sentence Length'].mean()))

#Let's plot boxplot to see distribution of length

plt.figure(figsize=(10,5))
sns.boxplot(data['Original Sentence Length'])
plt.show()

#Convert words to tokens

max_features=2000
tokenizer=Tokenizer(num_words=max_features, oov_token='<OOV>')
tokenizer.fit_on_texts(data['text'])

word_index=tokenizer.word_index
word_index

X = tokenizer.texts_to_sequences(data['text'])
X = pad_sequences(X, maxlen=100)

X

#Use Label Encoder to convert y

le=LabelEncoder()
y=le.fit_transform(data['label'])

y

#Use GloVe method

# load the GloVe vectors in a dictionary:

from tqdm import tqdm
import numpy as np

embeddings_index = {}
f = open(r'/content/drive/MyDrive/Colab Notebooks/Projects/NLP/glove.6B.50d.txt',encoding="utf8")

for line in tqdm(f):
    # Splitting the each line
    values = line.split()

    word = values[0]

    coefs = np.array(values[1:], dtype='float32')

    embeddings_index[word] = coefs

f.close()

print('Found %s word vectors.' % len(embeddings_index))

embeddings_index

#Creating embedding_matrix

all_embs = np.stack(list(embeddings_index.values()))
emb_mean,emb_std = all_embs.mean(), all_embs.std()
embed_size = all_embs.shape[1]

word_index = tokenizer.word_index
nb_words = min(max_features, len(word_index))
#change below line if computing normal stats is too slow
embedding_matrix = embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))
for word, i in word_index.items():
    if i >= max_features: continue
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None: embedding_matrix[i] = embedding_vector

embedding_vector

"""Question-

Model Construction and Training
Construct a suitable model and train the model on the dataset
"""

#Let's create neural network model

model=Sequential()
model.add(Embedding(nb_words, embed_size,weights=[embedding_matrix], input_shape=(X.shape[1],), trainable=True))
model.add(SpatialDropout1D(0.2))
model.add(GlobalAveragePooling1D())
model.add(Dense(64,activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(1,activation='sigmoid'))

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.001), loss='binary_crossentropy', metrics=['acc'])

model.summary()

from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint
es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=5)
mcp_save = ModelCheckpoint('.mdl_wts.h5', save_best_only=True, monitor='loss', mode='min')

# specifying the batch size
batch_size = 64

# fitting the model on the training data with 10 epochs
his = model.fit(X, y, epochs = 5, batch_size = batch_size, callbacks=[es,mcp_save], verbose = 'auto')

#Let me try using LSTM model

model2=Sequential()
model2.add(Embedding(nb_words, embed_size,weights=[embedding_matrix], input_shape=(X.shape[1],), trainable=True))
model2.add(SpatialDropout1D(0.2))
model2.add(Bidirectional(LSTM(64, return_sequences=False)))
model2.add(Dense(1, activation = 'sigmoid'))

model2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.001), loss='binary_crossentropy', metrics=['acc'])

model2.summary()

# specifying the batch size
batch_size = 64

# fitting the model on the training data with 10 epochs
his2 = model2.fit(X, y, epochs = 5, batch_size = batch_size, callbacks=[es,mcp_save], verbose = 'auto')

#Convert test dataset to tokens
X_test = tokenizer.texts_to_sequences(data_test['text'])
X_test = pad_sequences(X_test, maxlen=100)
y_test = le.transform(data_test['label'])

"""Question-

This criterion is linked to a Learning OutcomeModel Evaluation
Evaluate the model on data_test.csv and compute the evaluation/accuracy score
"""

model.evaluate(X_test, y_test)

ypred = model.predict(X_test)
ypred_label = [1 if prob >= 0.7 else 0 for prob in ypred]
ypred_label[:5]

print(classification_report(y_test, ypred_label, target_names = ['Not Sarcastic','Sarcastic']))

#Evaluate models

model2.evaluate(X_test, y_test)

ypred2 = model2.predict(X_test)

ypred_label2 = [1 if prob >= 0.7 else 0 for prob in ypred2]
ypred_label2[:5]

print(classification_report(y_test, ypred_label2, target_names = ['Not Sarcastic','Sarcastic']))

"""Conclusion-

I have used CNN and bidirectional LSTM as models. Accuracy is quite impressive in both the cases with accuracy more than 80%. Bidirectional LSTM is better model comparatively.
"""